{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f432854",
   "metadata": {},
   "source": [
    "\n",
    "# Tutorial 1: Learn about Configs\n",
    "\n",
    "We incorporate modular and inheritance design into our config system, which is convenient to conduct various experiments.\n",
    "If you wish to inspect the config file, you may run `python tools/misc/print_config.py /PATH/TO/CONFIG` to see the complete config.\n",
    "You may also pass `--cfg-options xxx.yyy=zzz` to see updated config.\n",
    "\n",
    "## Config File Structure\n",
    "\n",
    "There are 4 basic component types under `config/_base_`, datasets, models, schedules, default_runtime.\n",
    "Many methods could be easily constructed with one of each like DeepLabV3, PSPNet.\n",
    "The configs that are composed by components from `_base_` are called _primitive_.\n",
    "\n",
    "For all configs under the same folder, it is recommended to have only **one** _primitive_ config. All other configs should inherit from the _primitive_ config. In this way, the maximum of inheritance level is 3.\n",
    "\n",
    "For easy understanding, we recommend contributors to inherit from existing methods.\n",
    "For example, if some modification is made base on DeepLabV3, user may first inherit the basic DeepLabV3 structure by specifying `_base_ = ../deeplabv3/deeplabv3_r50-d8_4xb2-40k_cityscapes-512x1024.py`, then modify the necessary fields in the config files.\n",
    "\n",
    "If you are building an entirely new method that does not share the structure with any of the existing methods, you may create a folder `xxxnet` under `configs`,\n",
    "\n",
    "Please refer to [mmengine](https://mmengine.readthedocs.io/en/latest/tutorials/config.html) for detailed documentation.\n",
    "\n",
    "## Config Name Style\n",
    "\n",
    "We follow the below style to name config files. Contributors are advised to follow the same style.\n",
    "\n",
    "```text\n",
    "{algorithm name}_{model component names [component1]_[component2]_[...]}_{training settings}_{training dataset information}_{testing dataset information}\n",
    "```\n",
    "\n",
    "The file name is divided to five parts. All parts and components are connected with `_` and words of each part or component should be connected with `-`.\n",
    "\n",
    "- `{algorithm name}`: The name of the algorithm, such as `deeplabv3`, `pspnet`, etc.\n",
    "- `{model component names}`: Names of the components used in the algorithm such as backbone, head, etc. For example, `r50-d8` means using ResNet50 backbone and use output of backbone is 8 times downsampling as input.\n",
    "- `{training settings}`: Information of training settings such as batch size, augmentations, loss, learning rate scheduler, and epochs/iterations. For example: `4xb4-ce-linearlr-40K` means using 4-gpus x 4-images-per-gpu, CrossEntropy loss, Linear learning rate scheduler, and train 40K iterations.\n",
    "  Some abbreviations:\n",
    "  - `{gpu x batch_per_gpu}`: GPUs and samples per GPU. `bN` indicates N batch size per GPU. E.g. `8xb2` is the short term of 8-gpus x 2-images-per-gpu. And `4xb4` is used by default if not mentioned.\n",
    "  - `{schedule}`: training schedule, options are `20k`, `40k`, etc. `20k` and `40k` means 20000 iterations and 40000 iterations respectively.\n",
    "- `{training dataset information}`: Training dataset names like `cityscapes`, `ade20k`, etc, and input resolutions. For example: `cityscapes-768x768` means training on `cityscapes` dataset and the input shape is `768x768`.\n",
    "- `{testing dataset information}` (optional): Testing dataset name for models trained on one dataset but tested on another. If not mentioned, it means the model was trained and tested on the same dataset type.\n",
    "\n",
    "## An Example of PSPNet\n",
    "\n",
    "To help the users have a basic idea of a complete config and the modules in a modern semantic segmentation system,\n",
    "we make brief comments on the config of PSPNet using ResNet50V1c as the following.\n",
    "For more detailed usage and the corresponding alternative for each module, please refer to the API documentation.\n",
    "\n",
    "```python\n",
    "_base_ = [\n",
    "    '../_base_/models/pspnet_r50-d8.py', '../_base_/datasets/cityscapes.py',\n",
    "    '../_base_/default_runtime.py', '../_base_/schedules/schedule_40k.py'\n",
    "] # base config file which we build new config file on.\n",
    "crop_size = (512, 1024)\n",
    "data_preprocessor = dict(size=crop_size)\n",
    "model = dict(data_preprocessor=data_preprocessor)\n",
    "```\n",
    "\n",
    "`_base_/models/pspnet_r50-d8.py` is a basic model cfg file for PSPNet using ResNet50V1c\n",
    "\n",
    "```python\n",
    "# model settings\n",
    "norm_cfg = dict(type='SyncBN', requires_grad=True)  # Segmentation usually uses SyncBN\n",
    "data_preprocessor = dict(  # The config of data preprocessor, usually includes image normalization and augmentation.\n",
    "    type='SegDataPreProcessor',  # The type of data preprocessor.\n",
    "    mean=[123.675, 116.28, 103.53],  # Mean values used for normalizing the input images.\n",
    "    std=[58.395, 57.12, 57.375],  # Standard variance used for normalizing the input images.\n",
    "    bgr_to_rgb=True,  # Whether to convert image from BGR to RGB.\n",
    "    pad_val=0,  # Padding value of image.\n",
    "    seg_pad_val=255)  # Padding value of segmentation map.\n",
    "model = dict(\n",
    "    type='EncoderDecoder',  # Name of segmentor\n",
    "    data_preprocessor=data_preprocessor,\n",
    "    pretrained='open-mmlab://resnet50_v1c',  # The ImageNet pretrained backbone to be loaded\n",
    "    backbone=dict(\n",
    "        type='ResNetV1c',  # The type of backbone. Please refer to mmseg/models/backbones/resnet.py for details.\n",
    "        depth=50,  # Depth of backbone. Normally 50, 101 are used.\n",
    "        num_stages=4,  # Number of stages of backbone.\n",
    "        out_indices=(0, 1, 2, 3),  # The index of output feature maps produced in each stages.\n",
    "        dilations=(1, 1, 2, 4),  # The dilation rate of each layer.\n",
    "        strides=(1, 2, 1, 1),  # The stride of each layer.\n",
    "        norm_cfg=norm_cfg,  # The configuration of norm layer.\n",
    "        norm_eval=False,  # Whether to freeze the statistics in BN\n",
    "        style='pytorch',  # The style of backbone, 'pytorch' means that stride 2 layers are in 3x3 conv, 'caffe' means stride 2 layers are in 1x1 convs.\n",
    "        contract_dilation=True),  # When dilation > 1, whether contract first layer of dilation.\n",
    "    decode_head=dict(\n",
    "        type='PSPHead',  # Type of decode head. Please refer to mmseg/models/decode_heads for available options.\n",
    "        in_channels=2048,  # Input channel of decode head.\n",
    "        in_index=3,  # The index of feature map to select.\n",
    "        channels=512,  # The intermediate channels of decode head.\n",
    "        pool_scales=(1, 2, 3, 6),  # The avg pooling scales of PSPHead. Please refer to paper for details.\n",
    "        dropout_ratio=0.1,  # The dropout ratio before final classification layer.\n",
    "        num_classes=19,  # Number of segmentation class. Usually 19 for cityscapes, 21 for VOC, 150 for ADE20k.\n",
    "        norm_cfg=norm_cfg,  # The configuration of norm layer.\n",
    "        align_corners=False,  # The align_corners argument for resize in decoding.\n",
    "        loss_decode=dict(  # Config of loss function for the decode_head.\n",
    "            type='CrossEntropyLoss',  # Type of loss used for segmentation.\n",
    "            use_sigmoid=False,  # Whether use sigmoid activation for segmentation.\n",
    "            loss_weight=1.0)),  # Loss weight of decode_head.\n",
    "    auxiliary_head=dict(\n",
    "        type='FCNHead',  # Type of auxiliary head. Please refer to mmseg/models/decode_heads for available options.\n",
    "        in_channels=1024,  # Input channel of auxiliary head.\n",
    "        in_index=2,  # The index of feature map to select.\n",
    "        channels=256,  # The intermediate channels of decode head.\n",
    "        num_convs=1,  # Number of convs in FCNHead. It is usually 1 in auxiliary head.\n",
    "        concat_input=False,  # Whether concat output of convs with input before classification layer.\n",
    "        dropout_ratio=0.1,  # The dropout ratio before final classification layer.\n",
    "        num_classes=19,  # Number of segmentation class. Usually 19 for cityscapes, 21 for VOC, 150 for ADE20k.\n",
    "        norm_cfg=norm_cfg,  # The configuration of norm layer.\n",
    "        align_corners=False,  # The align_corners argument for resize in decoding.\n",
    "        loss_decode=dict(  # Config of loss function for the auxiliary_head.\n",
    "            type='CrossEntropyLoss',  # Type of loss used for segmentation.\n",
    "            use_sigmoid=False,  # Whether use sigmoid activation for segmentation.\n",
    "            loss_weight=0.4)),  # Loss weight of auxiliary_head.\n",
    "    # model training and testing settings\n",
    "    train_cfg=dict(),  # train_cfg is just a place holder for now.\n",
    "    test_cfg=dict(mode='whole'))  # The test mode, options are 'whole' and 'slide'. 'whole': whole image fully-convolutional test. 'slide': sliding crop window on the image.\n",
    "```\n",
    "\n",
    "`_base_/datasets/cityscapes.py` is the configuration file of the dataset\n",
    "\n",
    "```python\n",
    "# dataset settings\n",
    "dataset_type = 'CityscapesDataset'  # Dataset type, this will be used to define the dataset.\n",
    "data_root = 'data/cityscapes/'  # Root path of data.\n",
    "crop_size = (512, 1024)  # The crop size during training.\n",
    "train_pipeline = [  # Training pipeline.\n",
    "    dict(type='LoadImageFromFile'),  # First pipeline to load images from file path.\n",
    "    dict(type='LoadAnnotations'),  # Second pipeline to load annotations for current image.\n",
    "    dict(type='RandomResize',  # Augmentation pipeline that resize the images and their annotations.\n",
    "        scale=(2048, 1024),  # The scale of image.\n",
    "        ratio_range=(0.5, 2.0),  # The augmented scale range as ratio.\n",
    "        keep_ratio=True),  # Whether to keep the aspect ratio when resizing the image.\n",
    "    dict(type='RandomCrop',  # Augmentation pipeline that randomly crop a patch from current image.\n",
    "        crop_size=crop_size,  # The crop size of patch.\n",
    "        cat_max_ratio=0.75),  # The max area ratio that could be occupied by single category.\n",
    "    dict(type='RandomFlip',  # Augmentation pipeline that flip the images and their annotations\n",
    "        prob=0.5),  # The ratio or probability to flip\n",
    "    dict(type='PhotoMetricDistortion'),  # Augmentation pipeline that distort current image with several photo metric methods.\n",
    "    dict(type='PackSegInputs')  # Pack the inputs data for the semantic segmentation.\n",
    "]\n",
    "test_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),  # First pipeline to load images from file path\n",
    "    dict(type='Resize',  # Use resize augmentation\n",
    "        scale=(2048, 1024),  # Images scales for resizing.\n",
    "        keep_ratio=True),  # Whether to keep the aspect ratio when resizing the image.\n",
    "    # add loading annotation after ``Resize`` because ground truth\n",
    "    # does not need to do resize data transform\n",
    "    dict(type='LoadAnnotations'),  # Load annotations for semantic segmentation provided by dataset.\n",
    "    dict(type='PackSegInputs')  # Pack the inputs data for the semantic segmentation.\n",
    "]\n",
    "train_dataloader = dict(  # Train dataloader config\n",
    "    batch_size=2,  # Batch size of a single GPU\n",
    "    num_workers=2,  # Worker to pre-fetch data for each single GPU\n",
    "    persistent_workers=True,  # Shut down the worker processes after an epoch end, which can accelerate training speed.\n",
    "    sampler=dict(type='InfiniteSampler', shuffle=True),  # Randomly shuffle during training.\n",
    "    dataset=dict(  # Train dataset config\n",
    "        type=dataset_type,  # Type of dataset, refer to mmseg/datasets/ for details.\n",
    "        data_root=data_root,  # The root of dataset.\n",
    "        data_prefix=dict(\n",
    "            img_path='leftImg8bit/train', seg_map_path='gtFine/train'),  # Prefix for training data.\n",
    "        pipeline=train_pipeline)) # Processing pipeline. This is passed by the train_pipeline created before.\n",
    "val_dataloader = dict(\n",
    "    batch_size=1,  # Batch size of a single GPU\n",
    "    num_workers=4,  # Worker to pre-fetch data for each single GPU\n",
    "    persistent_workers=True,  # Shut down the worker processes after an epoch end, which can accelerate testing speed.\n",
    "    sampler=dict(type='DefaultSampler', shuffle=False),  # Not shuffle during validation and testing.\n",
    "    dataset=dict(  # Test dataset config\n",
    "        type=dataset_type,  # Type of dataset, refer to mmseg/datasets/ for details.\n",
    "        data_root=data_root,  # The root of dataset.\n",
    "        data_prefix=dict(\n",
    "            img_path='leftImg8bit/val', seg_map_path='gtFine/val'),  # Prefix for testing data.\n",
    "        pipeline=test_pipeline))  # Processing pipeline. This is passed by the test_pipeline created before.\n",
    "test_dataloader = val_dataloader\n",
    "# The metric to measure the accuracy. Here, we use IoUMetric.\n",
    "val_evaluator = dict(type='IoUMetric', iou_metrics=['mIoU'])\n",
    "test_evaluator = val_evaluator\n",
    "```\n",
    "\n",
    "`_base_/schedules/schedule_40k.py`\n",
    "\n",
    "```python\n",
    "# optimizer\n",
    "optimizer = dict(type='SGD', # Type of optimizers, refer to https://github.com/open-mmlab/mmengine/blob/main/mmengine/optim/optimizer/default_constructor.py for more details\n",
    "                lr=0.01,  # Learning rate of optimizers, see detail usages of the parameters in the documentation of PyTorch\n",
    "                momentum=0.9,  # Momentum\n",
    "                weight_decay=0.0005)  # Weight decay of SGD\n",
    "optim_wrapper = dict(type='OptimWrapper',  # Optimizer wrapper provides a common interface for updating parameters.\n",
    "                    optimizer=optimizer,  # Optimizer used to update model parameters.\n",
    "                    clip_grad=None)  # If ``clip_grad`` is not None, it will be the arguments of ``torch.nn.utils.clip_grad``.\n",
    "# learning policy\n",
    "param_scheduler = [\n",
    "    dict(\n",
    "        type='PolyLR',  # The policy of scheduler, also support Step, CosineAnnealing, Cyclic, etc. Refer to details of supported LrUpdater from https://github.com/open-mmlab/mmengine/blob/main/mmengine/optim/scheduler/lr_scheduler.py\n",
    "        eta_min=1e-4,  # Minimum learning rate at the end of scheduling.\n",
    "        power=0.9,  # The power of polynomial decay.\n",
    "        begin=0,  # Step at which to start updating the parameters.\n",
    "        end=40000,  # Step at which to stop updating the parameters.\n",
    "        by_epoch=False)  # Whether count by epoch or not.\n",
    "]\n",
    "# training schedule for 40k iteration\n",
    "train_cfg = dict(type='IterBasedTrainLoop', max_iters=40000, val_interval=4000)\n",
    "val_cfg = dict(type='ValLoop')\n",
    "test_cfg = dict(type='TestLoop')\n",
    "# default hooks\n",
    "default_hooks = dict(\n",
    "    timer=dict(type='IterTimerHook'),  # Log the time spent during iteration.\n",
    "    logger=dict(type='LoggerHook', interval=50, log_metric_by_epoch=False),  # Collect and write logs from different components of ``Runner``.\n",
    "    param_scheduler=dict(type='ParamSchedulerHook'),  # update some hyper-parameters in optimizer, e.g., learning rate.\n",
    "    checkpoint=dict(type='CheckpointHook', by_epoch=False, interval=4000),  # Save checkpoints periodically.\n",
    "    sampler_seed=dict(type='DistSamplerSeedHook'))  # Data-loading sampler for distributed training.\n",
    "```\n",
    "\n",
    "in `_base_/default_runtime.py`\n",
    "\n",
    "```python\n",
    "# Set the default scope of the registry to mmseg.\n",
    "default_scope = 'mmseg'\n",
    "# environment\n",
    "env_cfg = dict(\n",
    "    cudnn_benchmark=True,\n",
    "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),\n",
    "    dist_cfg=dict(backend='nccl'),\n",
    ")\n",
    "log_level = 'INFO'\n",
    "log_processor = dict(by_epoch=False)\n",
    "load_from = None  # Load checkpoint from file.\n",
    "resume = False  # Whether to resume from existed model.\n",
    "```\n",
    "\n",
    "These are all the configs for training and testing PSPNet, to load and parse them, we can use [Config](https://mmengine.readthedocs.io/en/latest/tutorials/config.html) implemented in [MMEngine](https://github.com/open-mmlab/mmengine)\n",
    "\n",
    "```python\n",
    "from mmengine.config import Config\n",
    "\n",
    "cfg = Config.fromfile('configs/pspnet/pspnet_r50-d8_4xb2-40k_cityscapes-512x1024.py')\n",
    "print(cfg.train_dataloader)\n",
    "```\n",
    "\n",
    "```shell\n",
    "{'batch_size': 2,\n",
    " 'num_workers': 2,\n",
    " 'persistent_workers': True,\n",
    " 'sampler': {'type': 'InfiniteSampler', 'shuffle': True},\n",
    " 'dataset': {'type': 'CityscapesDataset',\n",
    "  'data_root': 'data/cityscapes/',\n",
    "  'data_prefix': {'img_path': 'leftImg8bit/train',\n",
    "   'seg_map_path': 'gtFine/train'},\n",
    "  'pipeline': [{'type': 'LoadImageFromFile'},\n",
    "   {'type': 'LoadAnnotations'},\n",
    "   {'type': 'RandomResize',\n",
    "    'scale': (2048, 1024),\n",
    "    'ratio_range': (0.5, 2.0),\n",
    "    'keep_ratio': True},\n",
    "   {'type': 'RandomCrop', 'crop_size': (512, 1024), 'cat_max_ratio': 0.75},\n",
    "   {'type': 'RandomFlip', 'prob': 0.5},\n",
    "   {'type': 'PhotoMetricDistortion'},\n",
    "   {'type': 'PackSegInputs'}]}}\n",
    "```\n",
    "\n",
    "`cfg` is an instance of `mmengine.config.Config`, its interface is the same as a dict object and also allows access config values as attributes. See [config tutorial](https://mmengine.readthedocs.io/en/latest/tutorials/config.html) in [MMEngine](https://github.com/open-mmlab/mmengine) for more information.\n",
    "\n",
    "## FAQ\n",
    "\n",
    "### Ignore some fields in the base configs\n",
    "\n",
    "Sometimes, you may set `_delete_=True` to ignore some of the fields in base configs.\n",
    "See [config tutorial](https://mmengine.readthedocs.io/en/latest/tutorials/config.html) in [MMEngine](https://github.com/open-mmlab/mmengine) for simple illustration.\n",
    "\n",
    "In MMSegmentation, for example, if you would like to modify the backbone of PSPNet with the following config file `pspnet.py`:\n",
    "\n",
    "```python\n",
    "norm_cfg = dict(type='SyncBN', requires_grad=True)\n",
    "model = dict(\n",
    "    type='EncoderDecoder',\n",
    "    pretrained='torchvision://resnet50',\n",
    "    backbone=dict(\n",
    "        type='ResNetV1c',\n",
    "        depth=50,\n",
    "        num_stages=4,\n",
    "        out_indices=(0, 1, 2, 3),\n",
    "        dilations=(1, 1, 2, 4),\n",
    "        strides=(1, 2, 1, 1),\n",
    "        norm_cfg=norm_cfg,\n",
    "        norm_eval=False,\n",
    "        style='pytorch',\n",
    "        contract_dilation=True),\n",
    "    decode_head=dict(\n",
    "        type='PSPHead',\n",
    "        in_channels=2048,\n",
    "        in_index=3,\n",
    "        channels=512,\n",
    "        pool_scales=(1, 2, 3, 6),\n",
    "        dropout_ratio=0.1,\n",
    "        num_classes=19,\n",
    "        norm_cfg=norm_cfg,\n",
    "        align_corners=False,\n",
    "        loss_decode=dict(\n",
    "            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)))\n",
    "```\n",
    "\n",
    "Load and parse the config file `pspnet.py` in the code as follows:\n",
    "\n",
    "```python\n",
    "from mmengine.config import Config\n",
    "\n",
    "cfg = Config.fromfile('pspnet.py')\n",
    "print(cfg.model)\n",
    "```\n",
    "\n",
    "```shell\n",
    "{'type': 'EncoderDecoder',\n",
    " 'pretrained': 'torchvision://resnet50',\n",
    " 'backbone': {'type': 'ResNetV1c',\n",
    "  'depth': 50,\n",
    "  'num_stages': 4,\n",
    "  'out_indices': (0, 1, 2, 3),\n",
    "  'dilations': (1, 1, 2, 4),\n",
    "  'strides': (1, 2, 1, 1),\n",
    "  'norm_cfg': {'type': 'SyncBN', 'requires_grad': True},\n",
    "  'norm_eval': False,\n",
    "  'style': 'pytorch',\n",
    "  'contract_dilation': True},\n",
    " 'decode_head': {'type': 'PSPHead',\n",
    "  'in_channels': 2048,\n",
    "  'in_index': 3,\n",
    "  'channels': 512,\n",
    "  'pool_scales': (1, 2, 3, 6),\n",
    "  'dropout_ratio': 0.1,\n",
    "  'num_classes': 19,\n",
    "  'norm_cfg': {'type': 'SyncBN', 'requires_grad': True},\n",
    "  'align_corners': False,\n",
    "  'loss_decode': {'type': 'CrossEntropyLoss',\n",
    "   'use_sigmoid': False,\n",
    "   'loss_weight': 1.0}}}\n",
    "```\n",
    "\n",
    "`ResNet` and `HRNet` use different keywords to construct, write a new config file `hrnet.py` as follows:\n",
    "\n",
    "```python\n",
    "_base_ = 'pspnet.py'\n",
    "norm_cfg = dict(type='SyncBN', requires_grad=True)\n",
    "model = dict(\n",
    "    pretrained='open-mmlab://msra/hrnetv2_w32',\n",
    "    backbone=dict(\n",
    "        _delete_=True,\n",
    "        type='HRNet',\n",
    "        norm_cfg=norm_cfg,\n",
    "        extra=dict(\n",
    "            stage1=dict(\n",
    "                num_modules=1,\n",
    "                num_branches=1,\n",
    "                block='BOTTLENECK',\n",
    "                num_blocks=(4, ),\n",
    "                num_channels=(64, )),\n",
    "            stage2=dict(\n",
    "                num_modules=1,\n",
    "                num_branches=2,\n",
    "                block='BASIC',\n",
    "                num_blocks=(4, 4),\n",
    "                num_channels=(32, 64)),\n",
    "            stage3=dict(\n",
    "                num_modules=4,\n",
    "                num_branches=3,\n",
    "                block='BASIC',\n",
    "                num_blocks=(4, 4, 4),\n",
    "                num_channels=(32, 64, 128)),\n",
    "            stage4=dict(\n",
    "                num_modules=3,\n",
    "                num_branches=4,\n",
    "                block='BASIC',\n",
    "                num_blocks=(4, 4, 4, 4),\n",
    "                num_channels=(32, 64, 128, 256)))))\n",
    "```\n",
    "\n",
    "Load and parse the config file `hrnet.py` in the code as follows:\n",
    "\n",
    "```python\n",
    "from mmengine.config import Config\n",
    "cfg = Config.fromfile('hrnet.py')\n",
    "print(cfg.model)\n",
    "```\n",
    "\n",
    "```shell\n",
    "{'type': 'EncoderDecoder',\n",
    " 'pretrained': 'open-mmlab://msra/hrnetv2_w32',\n",
    " 'backbone': {'type': 'HRNet',\n",
    "  'norm_cfg': {'type': 'SyncBN', 'requires_grad': True},\n",
    "  'extra': {'stage1': {'num_modules': 1,\n",
    "    'num_branches': 1,\n",
    "    'block': 'BOTTLENECK',\n",
    "    'num_blocks': (4,),\n",
    "    'num_channels': (64,)},\n",
    "   'stage2': {'num_modules': 1,\n",
    "    'num_branches': 2,\n",
    "    'block': 'BASIC',\n",
    "    'num_blocks': (4, 4),\n",
    "    'num_channels': (32, 64)},\n",
    "   'stage3': {'num_modules': 4,\n",
    "    'num_branches': 3,\n",
    "    'block': 'BASIC',\n",
    "    'num_blocks': (4, 4, 4),\n",
    "    'num_channels': (32, 64, 128)},\n",
    "   'stage4': {'num_modules': 3,\n",
    "    'num_branches': 4,\n",
    "    'block': 'BASIC',\n",
    "    'num_blocks': (4, 4, 4, 4),\n",
    "    'num_channels': (32, 64, 128, 256)}}},\n",
    " 'decode_head': {'type': 'PSPHead',\n",
    "  'in_channels': 2048,\n",
    "  'in_index': 3,\n",
    "  'channels': 512,\n",
    "  'pool_scales': (1, 2, 3, 6),\n",
    "  'dropout_ratio': 0.1,\n",
    "  'num_classes': 19,\n",
    "  'norm_cfg': {'type': 'SyncBN', 'requires_grad': True},\n",
    "  'align_corners': False,\n",
    "  'loss_decode': {'type': 'CrossEntropyLoss',\n",
    "   'use_sigmoid': False,\n",
    "   'loss_weight': 1.0}}}\n",
    "```\n",
    "\n",
    "The `_delete_=True` would replace all old keys in `backbone` field with new keys.\n",
    "\n",
    "### Use intermediate variables in configs\n",
    "\n",
    "Some intermediate variables are used in the configs files, like `train_pipeline`/`test_pipeline` in datasets.\n",
    "It's worth noting that when modifying intermediate variables in the children configs, user need to pass the intermediate variables into corresponding fields again.\n",
    "For example, we would like to change multi scale strategy to train/test a PSPNet. `train_pipeline`/`test_pipeline` are intermediate variable we would like to modify.\n",
    "\n",
    "```python\n",
    "_base_ = '../pspnet/pspnet_r50-d8_4xb4-40k_cityscpaes-512x1024.py'\n",
    "crop_size = (512, 1024)\n",
    "img_norm_cfg = dict(\n",
    "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
    "train_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='LoadAnnotations'),\n",
    "    dict(type='RandomResize',\n",
    "         img_scale=(2048, 1024),\n",
    "         ratio_range=(1., 2.),\n",
    "         keep_ration=True),\n",
    "    dict(type='RandomCrop', crop_size=crop_size, cat_max_ratio=0.75),\n",
    "    dict(type='RandomFlip', flip_ratio=0.5),\n",
    "    dict(type='PhotoMetricDistortion'),\n",
    "    dict(type='PackSegInputs'),\n",
    "]\n",
    "test_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='Resize',\n",
    "        scale=(2048, 1024),\n",
    "        keep_ratio=True),\n",
    "    dict(type='LoadAnnotations'),\n",
    "    dict(type='PackSegInputs')\n",
    "]\n",
    "train_dataset=dict(\n",
    "        type=dataset_type,\n",
    "        data_root=data_root,\n",
    "        data_prefix=dict(\n",
    "            img_path='leftImg8bit/train', seg_map_path='gtFine/train'),\n",
    "        pipeline=train_pipeline)\n",
    "test_dataset=dict(\n",
    "        type=dataset_type,\n",
    "        data_root=data_root,\n",
    "        data_prefix=dict(\n",
    "            img_path='leftImg8bit/val', seg_map_path='gtFine/val'),\n",
    "        pipeline=test_pipeline)\n",
    "train_dataloader = dict(dataset=train_dataset)\n",
    "val_dataloader = dict(dataset=test_dataset)\n",
    "test_dataloader = val_dataloader\n",
    "```\n",
    "\n",
    "We first define the new `train_pipeline`/`test_pipeline` and pass them into `dataset`.\n",
    "\n",
    "Similarly, if we would like to switch from `SyncBN` to `BN` or `MMSyncBN`, we need to substitute every `norm_cfg` in the config.\n",
    "\n",
    "```python\n",
    "_base_ = '../pspnet/pspnet_r50-d8_4xb4-40k_cityscpaes-512x1024.py'\n",
    "norm_cfg = dict(type='BN', requires_grad=True)\n",
    "model = dict(\n",
    "    backbone=dict(norm_cfg=norm_cfg),\n",
    "    decode_head=dict(norm_cfg=norm_cfg),\n",
    "    auxiliary_head=dict(norm_cfg=norm_cfg))\n",
    "```\n",
    "\n",
    "## Modify config through script arguments\n",
    "\n",
    "In the [training script](https://github.com/open-mmlab/mmsegmentation/blob/1.x/tools/train.py) and the [testing script](https://github.com/open-mmlab/mmsegmentation/blob/1.x/tools/test.py), we support the script argument `--cfg-options`, it may help users override some settings in the used config, the key-value pair in `xxx=yyy` format will be merged into config file.\n",
    "\n",
    "For example, this is a simplified script `demo_script.py`:\n",
    "\n",
    "```python\n",
    "import argparse\n",
    "\n",
    "from mmengine.config import Config, DictAction\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Script Example')\n",
    "    parser.add_argument('config', help='train config file path')\n",
    "    parser.add_argument(\n",
    "        '--cfg-options',\n",
    "        nargs='+',\n",
    "        action=DictAction,\n",
    "        help='override some settings in the used config, the key-value pair '\n",
    "        'in xxx=yyy format will be merged into config file. If the value to '\n",
    "        'be overwritten is a list, it should be like key=\"[a,b]\" or key=a,b '\n",
    "        'It also allows nested list/tuple values, e.g. key=\"[(a,b),(c,d)]\" '\n",
    "        'Note that the quotation marks are necessary and that no white space '\n",
    "        'is allowed.')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    cfg = Config.fromfile(args.config)\n",
    "    if args.cfg_options is not None:\n",
    "        cfg.merge_from_dict(args.cfg_options)\n",
    "\n",
    "    print(cfg)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```\n",
    "\n",
    "An example config file `demo_config.py` as follows:\n",
    "\n",
    "```python\n",
    "backbone = dict(\n",
    "    type='ResNetV1c',\n",
    "    depth=50,\n",
    "    num_stages=4,\n",
    "    out_indices=(0, 1, 2, 3),\n",
    "    dilations=(1, 1, 2, 4),\n",
    "    strides=(1, 2, 1, 1),\n",
    "    norm_eval=False,\n",
    "    style='pytorch',\n",
    "    contract_dilation=True)\n",
    "```\n",
    "\n",
    "Run `demo_script.py`:\n",
    "\n",
    "```shell\n",
    "python demo_script.py demo_config.py\n",
    "```\n",
    "\n",
    "```shell\n",
    "Config (path: demo_config.py): {'backbone': {'type': 'ResNetV1c', 'depth': 50, 'num_stages': 4, 'out_indices': (0, 1, 2, 3), 'dilations': (1, 1, 2, 4), 'strides': (1, 2, 1, 1), 'norm_eval': False, 'style': 'pytorch', 'contract_dilation': True}}\n",
    "```\n",
    "\n",
    "Modify config through script arguments:\n",
    "\n",
    "```shell\n",
    "python demo_script.py demo_config.py --cfg-options backbone.depth=101\n",
    "```\n",
    "\n",
    "```shell\n",
    "Config (path: demo_config.py): {'backbone': {'type': 'ResNetV1c', 'depth': 101, 'num_stages': 4, 'out_indices': (0, 1, 2, 3), 'dilations': (1, 1, 2, 4), 'strides': (1, 2, 1, 1), 'norm_eval': False, 'style': 'pytorch', 'contract_dilation': True}}\n",
    "```\n",
    "\n",
    "- Update values of list/tuples.\n",
    "\n",
    "  If the value to be updated is a list or a tuple. For example, the config file `demo_config.py` sets `strides=(1, 2, 1, 1)` in `backbone`.\n",
    "  If you want to change this key, you may specify in two ways:\n",
    "\n",
    "  1. `--cfg-options backbone.strides=\"(1, 1, 1, 1)\"`. Note that the quotation mark \" is necessary to support list/tuple data types.\n",
    "\n",
    "     ```shell\n",
    "     python demo_script.py demo_config.py --cfg-options backbone.strides=\"(1, 1, 1, 1)\"\n",
    "     ```\n",
    "\n",
    "     ```shell\n",
    "     Config (path: demo_config.py): {'backbone': {'type': 'ResNetV1c', 'depth': 50, 'num_stages': 4, 'out_indices': (0, 1, 2, 3), 'dilations': (1, 1, 2, 4), 'strides': (1, 1, 1, 1), 'norm_eval': False, 'style': 'pytorch', 'contract_dilation': True}}\n",
    "     ```\n",
    "\n",
    "  2. `--cfg-options backbone.strides=1,1,1,1`. Note that **NO** white space is allowed in the specified value.\n",
    "     In addition, if the original type is tuple, it will be automatically converted to list after this way.\n",
    "\n",
    "     ```shell\n",
    "     python demo_script.py demo_config.py --cfg-options backbone.strides=1,1,1,1\n",
    "     ```\n",
    "\n",
    "     ```shell\n",
    "     Config (path: demo_config.py): {'backbone': {'type': 'ResNetV1c', 'depth': 50, 'num_stages': 4, 'out_indices': (0, 1, 2, 3), 'dilations': (1, 1, 2, 4), 'strides': [1, 1, 1, 1], 'norm_eval': False, 'style': 'pytorch', 'contract_dilation': True}}\n",
    "     ```\n",
    "\n",
    "```{note}\n",
    "    This modification method only supports modifying configuration items of string, int, float, boolean, None, list and tuple types.\n",
    "    More specifically, for list and tuple types, the elements inside them must also be one of the above seven types.\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
