## ğŸ“‹ Slayt DetaylarÄ±

| Slayt No | BÃ¶lÃ¼m | BaÅŸlÄ±k | Ä°Ã§erik (1 CÃ¼mle Ã–zet) | GÃ¶rsel Tasviri |
|---|---|---|---|---|
| 1 | GiriÅŸ | PyTorch ve Computer Vision EÄŸitim YolculuÄŸum | Sunum baÅŸlÄ±ÄŸÄ± ve genel tanÄ±tÄ±m | PyTorch, OpenMMLab ve MediaPipe logolarÄ±nÄ±n estetik, modern bir kolajÄ±. |
| 2 | GiriÅŸ | Ä°Ã§indekiler | Sunum akÄ±ÅŸÄ±nÄ± gÃ¶steren genel bakÄ±ÅŸ slaytÄ± | PyTorch, MMDetection, MMSegmentation ve MediaPipe baÅŸlÄ±klarÄ±nÄ±n ikonlarla listelendiÄŸi ÅŸÄ±k bir menÃ¼ tasarÄ±mÄ±. |
| 3 | BÃ¶lÃ¼m 1 | PyTorch Getting Started - Genel BakÄ±ÅŸ | FashionMNIST ile tensors, datasets, model building, autograd, optimization konularÄ±nÄ±n Ã¶zeti | FashionMNIST veri setinden Ã¶rnek gÃ¶rÃ¼ntÃ¼ler (ayakkabÄ±, Ã§anta) ve basit bir sinir aÄŸÄ± ÅŸemasÄ±. |
| 4 | BÃ¶lÃ¼m 1 | Custom Dataset NasÄ±l OluÅŸturulur? | Dataset sÄ±nÄ±fÄ±ndan miras alÄ±p __init__, __len__ ve __getitem__ metodlarÄ±nÄ± implement ederek Ã¶zel veri seti oluÅŸturma | KlasÃ¶r yapÄ±sÄ± -> __getitem__ -> Tensor dÃ¶nÃ¼ÅŸÃ¼mÃ¼nÃ¼ gÃ¶steren adÄ±m adÄ±m akÄ±ÅŸ ÅŸemasÄ±. |
| 5 | BÃ¶lÃ¼m 1 | Lambda Fonksiyonu ve One-Hot Encoding | Loss function ile target formatÄ±nÄ±n uyumlu olmasÄ± gerekir; MSELoss iÃ§in lambda ile one-hot encoding yapÄ±lÄ±r | TamsayÄ± etiket (5) ile One-Hot vektÃ¶r ([0,0,0,0,0,1...]) dÃ¶nÃ¼ÅŸÃ¼mÃ¼nÃ¼ gÃ¶steren ok diyagramÄ±. |
| 6 | BÃ¶lÃ¼m 2 | PyTorch Evrimi - 8 AdÄ±mda Ä°lerleme | NumPy'dan baÅŸlayÄ±p dynamic graphs'a kadar PyTorch ile model geliÅŸtirme evriminin gÃ¶sterimi | NumPy'dan Dynamic Graph'a giden 8 adÄ±mÄ± gÃ¶steren merdiven veya zaman Ã§izelgesi gÃ¶rseli. |

* 7 mmdetection giriÅŸ

* 8 mmdetection 7 main parts

MMDetection consists of 7 main parts, apis, structures, datasets, models, engine, evaluation and visualization.

* **apis** provides high-level APIs for model inference.
* **structures** provides data structures like ``bbox``, ``mask``, and ``DetDataSample``.
* **datasets** supports various dataset for ``object detection``, ``instance segmentation``, and ``panoptic segmentation``.
    * ***transforms** contains a lot of useful data augmentation transforms.
    * **samplers** defines different data loader sampling strategy.
* **models** is the most vital part for detectors and contains different components of a detector.
    * **detectors** defines all of the detection model classes.
    * **data_preprocessors** is for preprocessing the input data of the model.
    * **backbones** contains various ``backbone`` networks.
    * **necks** contains various ``neck`` components.
    * **dense_heads** contains various detection ``heads that perform dense predictions``.
    * **roi_heads** contains various detection ``heads that predict from RoIs``.
    * **seg_heads** contains various ``segmentation heads``.
    * **losses** contains various loss functions.
    * **task_modules** provides modules for detection tasks. E.g. ``assigners``, ``samplers``, ``box coders``, and ``prior generators``.
    * **layers** provides some basic neural network layers.
* **engine** is a part for runtime components.
    * **runner** provides extensions for MMEngineâ€™s runner.
    * **schedulers** provides schedulers for adjusting optimization hyperparameters.
    * **optimizers** provides optimizers and optimizer wrappers.
    * **hooks** provides various ``hooks`` of the runner.
* **evaluation** provides different metrics for evaluating model performance.
    * **metrics** contains different evaluation metrics.
    * **evaluators** provides evaluators for dataset evaluation.
* **visualization** is for visualizing detection results.

* 9 installation sÃ¼reci

**1. Anaconda Prompt iÃ§inde yeni bir ortam oluÅŸturun (Python 3.8 Ã¶nerilir):**
```bash
conda create --name openmmlab python=3.8 -y
conda activate openmmlab
```

**2. PyTorch'u yÃ¼kleyin (Versiyon 2.1.2 - Kritik AdÄ±m!):**
>En yeni sÃ¼rÃ¼mÃ¼ yÃ¼klemeyin, uyumluluk iÃ§in bu sÃ¼rÃ¼m ÅŸart.

```bash
pip install torch==2.1.2 torchvision==0.16.2 --index-url [https://download.pytorch.org/whl/cu121](https://download.pytorch.org/whl/cu121)
```

**3. OpenMIM ve MMEngine araÃ§larÄ±nÄ± kurun:**

```bash
pip install -U openmim
mim install mmengine
```

**4. MMCV'yi kurun (Versiyon 2.1.0):**
>PyTorch 2.1.2 ile tam uyumlu olan ve Windows'ta derleme hatasÄ± vermeyen sÃ¼rÃ¼m budur.

```bash
mim install "mmcv==2.1.0"
```

**5. MMDetection'Ä± kurun:**

```bash
mim install mmdet
```

* 10 mmdetecetion quickstart - 1: detinferencer - DetInferencer'Ä± baÅŸlatmak iÃ§in sadece **model adÄ±** yeterli. Weights otomatik indirilecek.

* 11 2: gÃ¶rsel indirme iÅŸlemi - C:\Users\eraye\Desktop\Eray\ee563\04_openmmlab\mmdetection\demo\test_images\demo.jpg

* 12 3: inferencer ile inference perform edebiliyoruz. C:\Users\eraye\Desktop\Eray\ee563\04_openmmlab\mmdetection\demo\outputs\vis\demo.jpg

* 13 6 gÃ¶rsel ve 6 model testi. C:\Users\eraye\Desktop\Eray\ee563\Presentation\images\6 images.png models = {
    'RTMDet-tiny': 'rtmdet_tiny_8xb32-300e_coco',       # En hÄ±zlÄ±
    'RTMDet-small': 'rtmdet_s_8xb32-300e_coco',         # Dengeli
    'RTMDet-large': 'rtmdet_l_8xb32-300e_coco',         # En doÄŸru RTMDet
    'Faster-RCNN': 'faster-rcnn_r50_fpn_1x_coco',       # Klasik two-stage
    'Mask-RCNN': 'mask-rcnn_r50_fpn_1x_coco',           # Instance segmentation
    'RetinaNet': 'retinanet_r50_fpn_1x_coco',           # Single-stage with focal loss
}

* 14 C:\Users\eraye\Desktop\Eray\ee563\Presentation\images\1.png

* 15 C:\Users\eraye\Desktop\Eray\ee563\Presentation\images\2.png

* 16 C:\Users\eraye\Desktop\Eray\ee563\Presentation\images\3.png

* 17 C:\Users\eraye\Desktop\Eray\ee563\Presentation\images\4.png

* 18 C:\Users\eraye\Desktop\Eray\ee563\Presentation\images\5.png

* 19 C:\Users\eraye\Desktop\Eray\ee563\Presentation\images\6.png

* 20 inferencer a direkt url linki veya batch (iÃ§inde birden fazla gÃ¶rsel olan liste) de verebiliyoruz

* 21 Ã§Ä±ktÄ±dan ÅŸu bilgilere eriÅŸebiliyoruz: Nesne 1: Label ID=13, Score=0.8762, BBox=[217.5468292236328, 172.820068359375, 457.94659423828125, 385.8176574707031]

* 22 mmcv.VideoReader ile modellere videolar da iletebiliyoruz input video: C:\Users\eraye\Desktop\Eray\ee563\04_openmmlab\mmdetection\demo\demo.mp4

* 23 video output C:\Users\eraye\Desktop\Eray\ee563\04_openmmlab\mmdetection\demo\output.mp4

* 24 Neden config var? (1 slide)

MMDetectionâ€™da â€œkod yazmaktan Ã§ok deneyi tarif ediyorsunâ€.
Model, dataset, eÄŸitim dÃ¶ngÃ¼sÃ¼, optimizer, scheduler, hookâ€™larâ€¦ hepsi tek bir yerde â€œdeney tarifiâ€ olarak duruyor.
AynÄ± altyapÄ±yla farklÄ± modelleri/datasetleri kolayca deÄŸiÅŸtirmenin ana yolu config.

* 25 Configâ€™in ana bloklarÄ± (1 slide)

model: backbone/neck/head + train_cfg/test_cfg (algoritmanÄ±n kendisi)
data_*: train_dataloader, val_dataloader, test_dataloader + pipeline(Load/Resize/Flip/Packâ€¦)
train_cfg, val_cfg, test_cfg: runner/loop tipi ve epoch/iter ayarlarÄ±
optim_wrapper: optimizer + gradient clip + (istersen AMP)
param_scheduler: LR warmup + MultiStep/Cosine vb.
default_hooks / custom_hooks: log, checkpoint, eval, visualization, seedâ€¦
env_cfg ve runtime: distributed backend, log level, resume/load_from vs.

* 26 En kritik fikir: Inheritance (base config) (1 slide)

Ã‡oÄŸu config â€œtam dosyaâ€ deÄŸil; bir veya birkaÃ§ base configâ€™ten miras alÄ±p sadece farkÄ± yazar.
Tipik pattern:
_base_ = [model_base, dataset_base, schedule_base, runtime_base]
BÃ¶ylece:
Tek satÄ±rla backbone veya dataset deÄŸiÅŸtirip deney Ã¼retirsin
Kopyala-yapÄ±ÅŸtÄ±r config ÅŸiÅŸmesini engellersin

* 27 KÃ¼Ã§Ã¼k deÄŸiÅŸiklik nasÄ±l yapÄ±lÄ±r? (demo/Ã¶rnek slide)

Ä°ki yol:
Config dosyasÄ±nda alanÄ± override etmek
Komut satÄ±rÄ±nda --cfg-options ile â€œin-placeâ€ deÄŸiÅŸtirmek
Ã–rnek anlatÄ±m:
â€œLRâ€™Ä± deÄŸiÅŸtir, batch sizeâ€™Ä± deÄŸiÅŸtir, pipelineâ€™a bir augment ekleâ€¦ hepsi config ileâ€

* 28 mask r cnn ile instance segmentation
- https://user-images.githubusercontent.com/40661020/143967081-c2552bed-9af2-46c4-ae44-5b3b74e5679f.png
- Mask R-CNN, â€œtwo-stage (iki aÅŸamalÄ±) detectorâ€ ailesinden bir instance segmentation modelidir: hem nesnenin kutusunu (bbox) hem de piksel seviyesinde maskesini Ã¼retir.

- Two-stage detector ne demek?
    - Stage 1 (Aday Ã¼retme / Proposal): Model Ã¶nce gÃ¶rÃ¼ntÃ¼ Ã¼zerinde â€œnesne olabilirâ€ dediÄŸi bÃ¶lgeler Ã¼retir. Mask R-CNNâ€™de bunu genelde RPN (Region Proposal Network) yapar. Ã‡Ä±kÄ±ÅŸ: Ã§ok sayÄ±da proposal (aday kutu).
    - Stage 2 (SÄ±nÄ±flandÄ±rma + ince ayar): Ãœretilen proposalâ€™lar Ã¼zerinden daha detaylÄ± iÅŸlem yapÄ±lÄ±r:
        - proposalâ€™lar feature mapâ€™ten RoIAlign ile kÄ±rpÄ±lÄ±p sabit boyuta getirilir,
        - bbox head ile sÄ±nÄ±f tahmini + kutu koordinatlarÄ±nÄ± refine eder,
        - Mask R-CNNâ€™e Ã¶zel olarak ayrÄ±ca mask head her RoI iÃ§in piksel seviyesinde maske Ã¼retir.

- Mask R-CNNâ€™in â€œMaskâ€ kÄ±smÄ±

Faster R-CNNâ€™e ek olarak ikinci aÅŸamada bir de mask branch vardÄ±r.
Bu sayede aynÄ± RoIâ€™den hem bbox hem mask Ã§Ä±kÄ±ÅŸÄ± alÄ±nÄ±r.

- ArtÄ±: Genelde tek-aÅŸamalÄ±lara gÃ¶re daha yÃ¼ksek doÄŸruluk (Ã¶zellikle zor sahnelerde).
- Eksi: Proposal + RoI iÅŸlemleri yÃ¼zÃ¼nden daha yavaÅŸ ve daha aÄŸÄ±rdÄ±r.

* 29 buradaki â€œmodel yapÄ±sÄ±â€ yorumu, Mask R-CNNâ€™in parÃ§alarÄ±nÄ±n ne iÅŸe yaradÄ±ÄŸÄ±nÄ± okuyucuya baÄŸlamak iÃ§in yazÄ±lmÄ±ÅŸ:

Backbone (ResNet)
Girdi gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼ alÄ±p Ã§ok katmanlÄ± â€œfeature mapâ€â€™lere Ã§eviren temel CNN omurgasÄ±dÄ±r. ResNet burada â€œÃ¶zellik Ã§Ä±karÄ±cÄ±â€ gibi Ã§alÄ±ÅŸÄ±r: kenar/kÃ¶ÅŸe gibi basitten baÅŸlayÄ±p daha soyut nesne Ã¶zelliklerine kadar temsil Ã¼retir.

Neck (FPN â€“ Feature Pyramid Network)
ResNet farklÄ± Ã§Ã¶zÃ¼nÃ¼rlÃ¼klerde Ã¶zellik haritalarÄ± Ã¼retir (erken katmanlar daha yÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼k, geÃ§ katmanlar daha dÃ¼ÅŸÃ¼k Ã§Ã¶zÃ¼nÃ¼rlÃ¼k ama daha semantik). FPN bu katmanlarÄ± birleÅŸtirip Ã§ok Ã¶lÃ§ekli (multi-scale) bir piramit oluÅŸturur.
Neden Ã¶nemli? KÃ¼Ã§Ã¼k nesne-bÃ¼yÃ¼k nesne gibi farklÄ± boyutlardaki objeleri daha iyi yakalamak iÃ§in.

RPN Head (Region Proposal Network)
FPNâ€™den gelen feature mapâ€™ler Ã¼zerinde â€œburada nesne olabilirâ€ dediÄŸi bÃ¶lgeler iÃ§in aday kutular (proposals) Ã¼retir.
Ä°ki temel Ã§Ä±ktÄ± verir:

Objectness skoru (bu kutu nesne mi arka plan mÄ±?)
BBox regression (kutuyu daha iyi oturtmak iÃ§in dÃ¼zeltme)
RoI Head (Stage-2 baÅŸlÄ±k)
RPNâ€™in Ã¼rettiÄŸi proposalâ€™lar ikinci aÅŸamaya gelir. Burada Ã¶nce RoIAlign ile her proposal bÃ¶lgesinin featureâ€™Ä± sabit boyuta kÄ±rpÄ±lÄ±p Ã§Ä±karÄ±lÄ±r (RoIPoolingâ€™e gÃ¶re daha hassas hizalama).
Sonra iki ayrÄ± â€œdalâ€ Ã§alÄ±ÅŸÄ±r:

Box head: proposalâ€™Ä± sÄ±nÄ±flandÄ±rÄ±r (hangi sÄ±nÄ±f?) ve bboxâ€™u daha da refine eder.
Mask head: aynÄ± proposal iÃ§in piksel seviyesinde maske tahmini Ã¼retir (instance segmentation kÄ±smÄ±).

Ã–zetle cÃ¼mle ÅŸunu demek istiyor: â€œModelin Ã§Ä±ktÄ±larÄ± (bbox + mask) tesadÃ¼fen deÄŸil; ResNetâ†’FPN ile gÃ¼Ã§lÃ¼ Ã¶zellik Ã§Ä±karÄ±p, RPN ile aday bÃ¶lgeler bulup, RoI Head iÃ§inde kutu + maske dallarÄ±yla sonucu Ã¼retiyor.â€

* 30 init_detector ve inference_detector methodlarÄ± ile inference yapÄ±yoruz
- C:\Users\eraye\Desktop\Eray\ee563\04_openmmlab\mmdetection\demo\demo.jpg
- DÃ¼ÅŸÃ¼k/orta seviye, â€œtek iÅŸâ€ API: Verilen model + image ile forward + postprocess yapÄ±p sonucu dÃ¶ndÃ¼rÃ¼r.
Modeli sen kurarsÄ±n: genelde Ã¶nce init_detector(config, checkpoint, device) Ã§aÄŸÄ±rÄ±p model alÄ±rsÄ±n, sonra inference_detector(model, img) dersin.

* 31 mask r cnn output
C:\Users\eraye\Desktop\Eray\ee563\Presentation\images\mask_output.png

* 32 custom bir dataset ile yeni bir detector train etmek
- 1 dataset i uyumlu hale getir
- 2 config dosyalarÄ±nÄ± revize et
- train et

There are three ways to support a new dataset in MMDetection:
  1. Reorganize the dataset into a COCO format
  2. Reorganize the dataset into a middle format
  3. Implement a new dataset

ilk 2si Ã¶neriliyor
mmdetection coco formatÄ± Ã¶neriyor, implement etmesi daha kolay

* 33 balloon dataseti kullanacaÄŸÄ±z
- C:\Users\eraye\Desktop\Eray\ee563\Presentation\images\balloon.png
- convert to VIA (VGG Image Annotator) format to coco format
- C:\Users\eraye\Desktop\Eray\ee563\Presentation\images\coco.png

* 34 ve
* 35 config ayarlarÄ±

```python
# Modify dataset classes and color
cfg.metainfo = {
    'classes': ('balloon', ),
    'palette': [
        (220, 20, 60),
    ]
}

# Modify dataset type and path
cfg.data_root = './data/balloon'

cfg.train_dataloader.dataset.ann_file = 'train/annotation_coco.json'
cfg.train_dataloader.dataset.data_root = cfg.data_root
cfg.train_dataloader.dataset.data_prefix.img = 'train/'
cfg.train_dataloader.dataset.metainfo = cfg.metainfo

cfg.val_dataloader.dataset.ann_file = 'val/annotation_coco.json'
cfg.val_dataloader.dataset.data_root = cfg.data_root
cfg.val_dataloader.dataset.data_prefix.img = 'val/'
cfg.val_dataloader.dataset.metainfo = cfg.metainfo

cfg.test_dataloader = cfg.val_dataloader

# Modify metric config
cfg.val_evaluator.ann_file = cfg.data_root+'/'+'val/annotation_coco.json'
cfg.test_evaluator = cfg.val_evaluator

# Modify num classes of the model in box head and mask head
cfg.model.roi_head.bbox_head.num_classes = 1
cfg.model.roi_head.mask_head.num_classes = 1

# We can still the pre-trained Mask RCNN model to obtain a higher performance
cfg.load_from = 'checkpoints/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'

# Set up working dir to save files and logs.
cfg.work_dir = './tutorial_exps'


# We can set the evaluation interval to reduce the evaluation times
cfg.train_cfg.val_interval = 3
# We can set the checkpoint saving interval to reduce the storage cost
cfg.default_hooks.checkpoint.interval = 3

# The original learning rate (LR) is set for 8-GPU training.
# We divide it by 8 since we only use one GPU.
cfg.optim_wrapper.optimizer.lr = 0.02 / 8
cfg.default_hooks.logger.interval = 10


# Set seed thus the results are more reproducible
# cfg.seed = 0
set_random_seed(0, deterministic=False)

# We can also use tensorboard to log the training process
cfg.visualizer.vis_backends.append({"type":'TensorboardVisBackend'})
```

* 36 runner ile training

mmengine.runner.Runner ile training â€” artÄ±larÄ±

Tek Ã§atÄ± altÄ±nda eÄŸitim orkestrasyonu: train/val/test loopâ€™larÄ±nÄ±, epoch/iter kontrolÃ¼nÃ¼ ve val_interval gibi akÄ±ÅŸÄ± configâ€™ten yÃ¶netir; kod tarafÄ±nda â€œtraining scriptâ€ yazma ihtiyacÄ± azalÄ±r.
Config-first yaklaÅŸÄ±m: Optimizer (optim_wrapper), scheduler (param_scheduler), dataloader, hookâ€™lar vb. her ÅŸey config Ã¼zerinden yÃ¶netildiÄŸi iÃ§in deneyi tekrarlamak/versiyonlamak kolaylaÅŸÄ±r.
Hook sistemiyle geniÅŸletilebilirlik: Logging, checkpoint, evaluation, visualization gibi iÅŸleri â€œhookâ€ olarak standart bir ÅŸekilde tak-Ã§Ä±kar yaparsÄ±n; eÄŸitim kodun sade kalÄ±r.
Checkpoint & resume kolaylÄ±ÄŸÄ±: default_hooks.checkpoint + resume/load_from ile kesintiden devam, en iyi modeli saklama, periyodik kayÄ±t gibi pratikler hazÄ±r gelir.
DaÄŸÄ±tÄ±k eÄŸitim uyumu: AynÄ± Runner tasarÄ±mÄ± single GPU/CPUâ€™dan distributed ortamlara daha doÄŸal taÅŸÄ±nÄ±r (altyapÄ± MMEngine tarafÄ±nda).
Standart log/metric akÄ±ÅŸÄ±: LogProcessor/LoggerHook ile metriklerin dÃ¼zenli toplanmasÄ± ve raporlanmasÄ± daha tutarlÄ± olur.

* 37 mmdetection sonu, neler Ã¶ÄŸrendik

* 38 mmsegmentation giriÅŸ ve installation
```bash
pip install "mmsegmentation>=1.0.0"
```

* 39 init_model inference_model kullanarak demo
- pspnet_r50 modeli kullandÄ±k
- input: C:\Users\eraye\Desktop\Eray\ee563\04_openmmlab\mmsegmentation\demo\demo.png
- output: C:\Users\eraye\Desktop\Eray\ee563\04_openmmlab\mmsegmentation\outputs\result.jpg

* 40 video iÅŸleme
- input: C:\Users\eraye\Desktop\Eray\ee563\04_openmmlab\mmsegmentation\demo\demo.mp4
- output: C:\Users\eraye\Desktop\Eray\ee563\04_openmmlab\mmsegmentation\outputs\video_result.mp4

* 41 config

1) Config = â€œdeneyin tarifiâ€
MMSegâ€™de model, dataset, eÄŸitim scheduleâ€™Ä± ve runtime ayarlarÄ±nÄ±n tamamÄ± bir config dosyasÄ±nda toplanÄ±r.
AynÄ± kodu deÄŸiÅŸtirmeden farklÄ± deneyleri sadece config deÄŸiÅŸtirerek Ã§alÄ±ÅŸtÄ±rÄ±rsÄ±n.
2) ModÃ¼ler yapÄ± + kalÄ±tÄ±m (inheritance)
Configâ€™ler genelde parÃ§a parÃ§a gelir ve bir ana config bunlarÄ± â€œmiras alÄ±râ€:

models/ â†’ mimari (backbone, decode_head, loss, num_classesâ€¦)
datasets/ â†’ veri yolu, pipeline, augmentation
schedules/ â†’ optimizer + LR scheduler + max_iters
default_runtime/ â†’ log, checkpoint, seed, env
Ã–rnek mantÄ±k:

â€œPSPNet + Cityscapes + 40k schedule + default runtimeâ€ = tek config.

* 42 CONFIG
* 43 CONFIG

* 44 segmentation Ã§eÅŸitleri

Semantic segmentation: Her piksele sÄ±nÄ±f etiketi (aynÄ± sÄ±nÄ±ftaki tÃ¼m nesneler birleÅŸik).
Instance segmentation: Her nesneyi ayrÄ± maske olarak ayÄ±rÄ±r (aynÄ± sÄ±nÄ±fta birden Ã§ok obje ayrÄ±).
Panoptic segmentation: Semantic + instance birlikte (things = instance, stuff = semantic).

* 45 finetune a semantic segmentation model on a new dataset
- 1 dataset indir ve uygun hale getir
- 2 config ayarlarÄ±
- 3 training

* 46 stanford background datasetini kullandÄ±k
- C:\Users\eraye\Desktop\Eray\ee563\Presentation\images\stanford.png
- C:\Users\eraye\Desktop\Eray\ee563\Presentation\images\stanford2.png
- class ve palette belirledik
```python
classes = ('sky', 'tree', 'road', 'grass', 'water', 'bldg', 'mntn', 'fg obj')
palette = [[128, 128, 128], [129, 127, 38], [120, 69, 125], [53, 125, 34], 
           [0, 11, 123], [118, 20, 12], [122, 81, 25], [241, 134, 51]]
```
* 47 yeni bir class tanÄ±mlamak?
- bu tutorial kendi datasetini nasÄ±l implement edersin diye yola Ã§Ä±ktÄ±ÄŸÄ± iÃ§in bÃ¶yle bir Ã¶rnek vermiÅŸ
- Kendi dataset sÄ±nÄ±fÄ±n, ama iÅŸi kolay olsun diye MMSegâ€™in hazÄ±r temel sÄ±nÄ±fÄ±ndan tÃ¼retiyorsun. 
- Configâ€™ten Ã§aÄŸrÄ±labilir bir dataset sÄ±nÄ±fÄ± tanÄ±mlÄ±yorsun; sÄ±nÄ±f isimleri ve renk paletini de meta bilgi olarak ekliyorsun.

* 48 ve
* 49 CONFIG Ã–RNEÄÄ°

```python
# Since we use only one GPU, BN is used instead of SyncBN
cfg.norm_cfg = dict(type='BN', requires_grad=True)
cfg.crop_size = (256, 256)
cfg.model.data_preprocessor.size = cfg.crop_size
cfg.model.backbone.norm_cfg = cfg.norm_cfg
cfg.model.decode_head.norm_cfg = cfg.norm_cfg
cfg.model.auxiliary_head.norm_cfg = cfg.norm_cfg
# modify num classes of the model in decode/auxiliary head
cfg.model.decode_head.num_classes = 8
cfg.model.auxiliary_head.num_classes = 8

# Modify dataset type and path
cfg.dataset_type = 'StanfordBackgroundDataset'
cfg.data_root = data_root

cfg.train_dataloader.batch_size = 8

cfg.train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='RandomResize', scale=(320, 240), ratio_range=(0.5, 2.0), keep_ratio=True),
    dict(type='RandomCrop', crop_size=cfg.crop_size, cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PackSegInputs')
]

cfg.test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='Resize', scale=(320, 240), keep_ratio=True),
    # add loading annotation after ``Resize`` because ground truth
    # does not need to do resize data transform
    dict(type='LoadAnnotations'),
    dict(type='PackSegInputs')
]


cfg.train_dataloader.dataset.type = cfg.dataset_type
cfg.train_dataloader.dataset.data_root = cfg.data_root
cfg.train_dataloader.dataset.data_prefix = dict(img_path=img_dir, seg_map_path=ann_dir)
cfg.train_dataloader.dataset.pipeline = cfg.train_pipeline
cfg.train_dataloader.dataset.ann_file = 'splits/train.txt'

cfg.val_dataloader.dataset.type = cfg.dataset_type
cfg.val_dataloader.dataset.data_root = cfg.data_root
cfg.val_dataloader.dataset.data_prefix = dict(img_path=img_dir, seg_map_path=ann_dir)
cfg.val_dataloader.dataset.pipeline = cfg.test_pipeline
cfg.val_dataloader.dataset.ann_file = 'splits/val.txt'

cfg.test_dataloader = cfg.val_dataloader

# Ã‡oklu iÅŸlem (multiprocessing) hatasÄ±nÄ± Ã¶nlemek iÃ§in worker sayÄ±sÄ±nÄ± 0 yapÄ±yoruz
cfg.train_dataloader.num_workers = 0
cfg.val_dataloader.num_workers = 0
cfg.test_dataloader.num_workers = 0

# num_workers=0 olduÄŸunda persistent_workers=False olmak zorundadÄ±r
cfg.train_dataloader.persistent_workers = False
cfg.val_dataloader.persistent_workers = False
cfg.test_dataloader.persistent_workers = False


# Load the pretrained weights
cfg.load_from = 'models/pspnet_r50-d8_512x1024_40k_cityscapes_20200605_003338-2966598c.pth'

# Set up working dir to save files and logs.
cfg.work_dir = './work_dirs/tutorial'

cfg.train_cfg.max_iters = 200
cfg.train_cfg.val_interval = 200
cfg.default_hooks.logger.interval = 10
cfg.default_hooks.checkpoint.interval = 200

# Set seed to facilitate reproducing the result
cfg['randomness'] = dict(seed=0)

# Let's have a look at the final config used for training
print(f'Config:\n{cfg.pretty_text}')
```

* 50 runner ile train. ve inference Ã¶rneÄŸi
- input: C:\Users\eraye\Desktop\Eray\ee563\Presentation\images\stanford3.png
- output: C:\Users\eraye\Desktop\Eray\ee563\Presentation\images\stanford4.png

* 51 mmsegmentation bitti, neler Ã¶ÄŸrendik

* 52 mediapipe
- mediapipe nedir?
- 3 Ã¶rnek: face-detection, face-landmark, pose-landmark

* 53 face-detection 1
Slayt 1 â€” MediaPipe Face Detection: Kurulum + Pipeline MantÄ±ÄŸÄ±
AmaÃ§: GÃ¶rÃ¼ntÃ¼/video frameâ€™lerinde yÃ¼z(ler)i tespit etmek
Ã‡Ä±ktÄ±: bbox + 6 landmark (gÃ¶zler, burun ucu, aÄŸÄ±z, tragion noktalarÄ±) + confidence
Model: BlazeFace (detector.tflite)
Notebookâ€™ta linkten indiriliyor
AkÄ±ÅŸ (yÃ¼ksek seviye):
Model dosyasÄ±nÄ± indir
FaceDetectorâ€™Ä± modelle baÅŸlat
GÃ¶rÃ¼ntÃ¼yÃ¼ MediaPipe formatÄ±nda yÃ¼kle
detect() ile inference
Sonucu bbox + keypoint olarak Ã§iz
- input: C:\Users\eraye\Desktop\Eray\ee563\Presentation\images\face_detection1.png

* 54 face-detection 2
Slayt 2 â€” Kod AdÄ±mlarÄ±: Detect + GÃ¶rselleÅŸtirme
1) Detector oluÅŸturma
BaseOptions(model_asset_path='detector.tflite')
FaceDetectorOptions(...)
FaceDetector.create_from_options(options)
2) GÃ¶rÃ¼ntÃ¼yÃ¼ yÃ¼kleme ve inference
image = mp.Image.create_from_file(IMAGE_FILE)
detection_result = detector.detect(image)
3) Sonucu Ã§izdirme (visualize fonksiyonu)
detection_result.detections Ã¼zerinde dÃ¶ngÃ¼
Her detection iÃ§in:
bounding_box â†’ cv2.rectangle(...)
keypoints (normalize [0,1]) â†’ piksele Ã§evir â†’ cv2.circle(...)
score/label â†’ cv2.putText(...)
Not: image.numpy_view() zaten RGB â†’ matplotlib ile direkt gÃ¶steriliyor
4) Elde edilen Ã§Ä±ktÄ±
Konsolda print(detection_result) ile tÃ¼m bbox/landmark/score bilgileri gÃ¶rÃ¼lebilir
output: C:\Users\eraye\Desktop\Eray\ee563\Presentation\images\face_detection2.png

* 55 face_landmark 1
Slayt 1 â€” MediaPipe Face Landmarker: Ne yapÄ±yor, ne Ã¼retir?
AmaÃ§: YÃ¼zÃ¼ bulup face mesh (478 landmark) Ã§Ä±karmak (2D/3D landmark koordinatlarÄ±).
Ek Ã§Ä±ktÄ±lar (opsiyonel):
Blendshapes (52 skor): mimik/ifade katsayÄ±larÄ± (Ã¶rn. smile, eyeBlink vb.)
Facial transformation matrices: efekti/3D yÃ¼z modelini doÄŸru hizalamak iÃ§in dÃ¶nÃ¼ÅŸÃ¼m matrisleri
Model paketi (face_landmarker.task): iÃ§erde birden fazla model var
Ã¶nce face detection, sonra landmark/mesh, sonra blendshape aÅŸamasÄ±.
- input: C:\Users\eraye\Desktop\Eray\ee563\Presentation\images\face_landmark1.png

* 56 face_landmark 2
1) Landmarkerâ€™Ä± baÅŸlatma
BaseOptions(model_asset_path='face_landmarker.task')
FaceLandmarkerOptions(..., num_faces=1, output_face_blendshapes=True, output_facial_transformation_matrixes=True)
FaceLandmarker.create_from_options(options)
2) GÃ¶rÃ¼ntÃ¼ yÃ¼kle + inference
image = mp.Image.create_from_file("image.png")
detection_result = detector.detect(image)
3) GÃ¶rselleÅŸtirme (mesh Ã§izimi)
draw_landmarks_on_image(...) iÃ§inde drawing_utils.draw_landmarks ile:
Tesselation (mesh Ã¼Ã§gen aÄŸÄ±)
Contours (yÃ¼z hatlarÄ±)
Left/Right iris baÄŸlantÄ±larÄ± Ã§izilir
4) SonuÃ§larÄ± okumak
detection_result.face_landmarks â†’ landmark listeleri
detection_result.face_blendshapes[0] â†’ bar plot ile ifade skorlarÄ±
detection_result.facial_transformation_matrixes â†’ 3D hizalama iÃ§in matris Ã§Ä±ktÄ±sÄ±
- output: C:\Users\eraye\Desktop\Eray\ee563\Presentation\images\face_landmark2.png

* 57 pose landmark 1
Slayt 1 â€” Pose Landmarker: Ne yapar, ne Ã¼retir?
AmaÃ§: GÃ¶rÃ¼ntÃ¼/videoâ€™da insan pozunu tespit edip 33 vÃ¼cut landmarkâ€™Ä± Ã§Ä±karmak.
Ã‡Ä±ktÄ±lar:
pose_landmarks: gÃ¶rÃ¼ntÃ¼ye gÃ¶re normalize (0â€“1) koordinatlar
(opsiyonel) pose_world_landmarks: 3D world koordinatlarÄ±
(opsiyonel) segmentation_masks: kiÅŸi silueti iÃ§in pose mask
Model paketi: pose_landmarker_heavy.task (detector + landmarker pipelineâ€™Ä± tek dosyada)

- input: 
C:\Users\eraye\Desktop\Eray\ee563\Presentation\images\pose_landmark1.png


* 58 pose landmark 2
Slayt 2 â€” Kod akÄ±ÅŸÄ±: Kurulum â†’ Detect â†’ Ã‡iz â†’ Maskeyi gÃ¶rselleÅŸtir
1) Nesneyi oluÅŸturma
BaseOptions(model_asset_path='pose_landmarker_heavy.task')
PoseLandmarkerOptions(output_segmentation_masks=True)
PoseLandmarker.create_from_options(options)
2) Inference
image = mp.Image.create_from_file("image1.jpg")
detection_result = detector.detect(image)
3) Landmark Ã§izimi
draw_landmarks_on_image(image.numpy_view(), detection_result)
drawing_utils.draw_landmarks(..., connections=POSE_LANDMARKS) ile skeleton baÄŸlantÄ±larÄ±
4) Segmentation mask hazÄ±rlama
segmentation_mask = detection_result.segmentation_masks[0].numpy_view() (float 0â€“1)
np.squeeze ile (H,W) yap
*255 + astype(np.uint8) ile gÃ¶rÃ¼ntÃ¼lenebilir maske
np.stack([mask]*3, axis=-1) ile 3 kanal (matplotlib/cv2 iÃ§in)

- output: C:\Users\eraye\Desktop\Eray\ee563\Presentation\images\pose_landmark2.png

* 59 mediapipe sonu, ne Ã¶ÄŸrendik

* 60 kapanÄ±ÅŸ